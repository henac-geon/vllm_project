# ============================================
# PDF ë…¼ë¬¸ â†’ ë°ì´í„°ì…‹ â†’ LoRA í•™ìŠµ íŒŒì´í”„ë¼ì¸
# ============================================
# 
# ğŸ“Œ ì‚¬ìš©ë²•:
# 1. Google Colabì—ì„œ ì‹¤í–‰
# 2. PDF íŒŒì¼ ì—…ë¡œë“œ
# 3. ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
#
# ============================================

# %% [Cell 1] íŒ¨í‚¤ì§€ ì„¤ì¹˜
# ============================================
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps trl peft accelerate bitsandbytes
!pip install pdfplumber pypdf sentence-transformers

# %% [Cell 2] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================
import pdfplumber
import json
import re
import os
from tqdm import tqdm
from datasets import Dataset
from google.colab import files

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!")

# %% [Cell 3] PDF íŒŒì¼ ì—…ë¡œë“œ
# ============================================
print("ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”...")
uploaded = files.upload()

# ì—…ë¡œë“œëœ PDF íŒŒì¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
pdf_files = [f for f in uploaded.keys() if f.endswith('.pdf')]
print(f"âœ… ì—…ë¡œë“œëœ PDF: {pdf_files}")

# %% [Cell 4] PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ============================================

def extract_text_from_pdf(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    full_text = ""
    
    with pdfplumber.open(pdf_path) as pdf:
        print(f"ğŸ“– ì´ {len(pdf.pages)} í˜ì´ì§€")
        
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                full_text += f"\n\n--- Page {i+1} ---\n\n"
                full_text += text
    
    return full_text


def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r' {2,}', ' ', text)
    # í•˜ì´í”ˆìœ¼ë¡œ ëŠê¸´ ë‹¨ì–´ ì—°ê²°
    text = re.sub(r'(\w+)-\n(\w+)', r'\1\2', text)
    return text.strip()


# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
all_texts = {}
for pdf_file in pdf_files:
    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file}")
    raw_text = extract_text_from_pdf(pdf_file)
    cleaned_text = clean_text(raw_text)
    all_texts[pdf_file] = cleaned_text
    print(f"   ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")

# ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
combined_text = "\n\n".join(all_texts.values())
print(f"\nâœ… ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ì´ {len(combined_text)} ë¬¸ì")

# %% [Cell 5] í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
# ============================================

def split_into_chunks(text, chunk_size=1000, overlap=100):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„ë¦¬
    paragraphs = text.split('\n\n')
    
    current_chunk = ""
    for para in paragraphs:
        if len(current_chunk) + len(para) < chunk_size:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\n\n"
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def split_into_sections(text):
    """ë…¼ë¬¸ ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬ (Abstract, Introduction ë“±)"""
    section_patterns = [
        r'(?i)(abstract)',
        r'(?i)(introduction)',
        r'(?i)(related work|background)',
        r'(?i)(method|methodology|approach)',
        r'(?i)(experiment|evaluation|result)',
        r'(?i)(discussion)',
        r'(?i)(conclusion)',
        r'(?i)(reference|bibliography)',
    ]
    
    sections = {}
    current_section = "header"
    current_text = ""
    
    lines = text.split('\n')
    
    for line in lines:
        found_section = False
        for pattern in section_patterns:
            if re.match(pattern, line.strip()):
                if current_text:
                    sections[current_section] = current_text.strip()
                current_section = line.strip().lower()
                current_text = ""
                found_section = True
                break
        
        if not found_section:
            current_text += line + "\n"
    
    if current_text:
        sections[current_section] = current_text.strip()
    
    return sections


# ì²­í¬ ìƒì„±
chunks = split_into_chunks(combined_text, chunk_size=800)
print(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")

# ì„¹ì…˜ ë¶„ë¦¬ ì‹œë„
sections = split_into_sections(combined_text)
print(f"âœ… ë°œê²¬ëœ ì„¹ì…˜: {list(sections.keys())}")

# %% [Cell 6] ë°ì´í„°ì…‹ ìƒì„± - ë°©ë²• ì„ íƒ
# ============================================

# ğŸ“Œ ë°©ë²• 1: ê·œì¹™ ê¸°ë°˜ Q&A ìƒì„±
def create_qa_dataset_rule_based(chunks, paper_title="ë…¼ë¬¸"):
    """ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ Q&A ë°ì´í„°ì…‹ ìƒì„±"""
    dataset = []
    
    qa_templates = [
        {
            "instruction": "ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input_prefix": "",
            "output_prefix": "ìš”ì•½: "
        },
        {
            "instruction": "ë‹¤ìŒ ë‚´ìš©ì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "input_prefix": "",
            "output_prefix": "í•µì‹¬ í¬ì¸íŠ¸: "
        },
        {
            "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
            "input_prefix": "í…ìŠ¤íŠ¸: ",
            "output_prefix": ""
        },
    ]
    
    for i, chunk in enumerate(chunks):
        if len(chunk) < 100:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ìŠ¤í‚µ
            continue
        
        # ìš”ì•½ íƒœìŠ¤í¬
        dataset.append({
            "instruction": "ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": chunk,
            "output": f"ì´ ë‚´ìš©ì€ {paper_title}ì˜ ì¼ë¶€ë¡œ, " + chunk[:200] + "..."
        })
        
        # ì„¤ëª… íƒœìŠ¤í¬
        dataset.append({
            "instruction": "ë‹¤ìŒ í•™ìˆ  ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "input": chunk,
            "output": f"ì‰½ê²Œ ì„¤ëª…í•˜ë©´, {chunk[:300]}..."
        })
    
    return dataset


# ğŸ“Œ ë°©ë²• 2: LLMì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ Q&A ìƒì„± (ê¶Œì¥)
def create_qa_dataset_with_llm(chunks, model, tokenizer, alpaca_prompt):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ Q&A ë°ì´í„°ì…‹ ìƒì„±"""
    from unsloth import FastLanguageModel
    
    dataset = []
    FastLanguageModel.for_inference(model)
    
    for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc="Q&A ìƒì„± ì¤‘"):
        if len(chunk) < 100:
            continue
        
        # ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸
        question_prompt = alpaca_prompt.format(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì´ ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 3ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê° ì§ˆë¬¸ì€ ìƒˆ ì¤„ì— ì‘ì„±í•˜ì„¸ìš”.",
            chunk[:1500],
            ""
        )
        
        inputs = tokenizer([question_prompt], return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
        questions_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì§ˆë¬¸ íŒŒì‹±
        if "### Response:" in questions_text:
            questions_text = questions_text.split("### Response:")[-1].strip()
        
        questions = [q.strip() for q in questions_text.split('\n') if q.strip() and '?' in q]
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        for question in questions[:2]:  # ì§ˆë¬¸ 2ê°œë§Œ ì‚¬ìš©
            answer_prompt = alpaca_prompt.format(
                "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
                f"í…ìŠ¤íŠ¸: {chunk[:1500]}\n\nì§ˆë¬¸: {question}",
                ""
            )
            
            inputs = tokenizer([answer_prompt], return_tensors="pt").to("cuda")
            outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3)
            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if "### Response:" in answer:
                answer = answer.split("### Response:")[-1].strip()
            
            dataset.append({
                "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
                "input": f"í…ìŠ¤íŠ¸: {chunk}\n\nì§ˆë¬¸: {question}",
                "output": answer
            })
    
    return dataset


# ğŸ“Œ ë°©ë²• 3: ì§ì ‘ instruction-output ìŒ ìƒì„±
def create_instruction_dataset(sections):
    """ë…¼ë¬¸ ì„¹ì…˜ë³„ë¡œ instruction ë°ì´í„°ì…‹ ìƒì„±"""
    dataset = []
    
    section_instructions = {
        "abstract": [
            ("ì´ ë…¼ë¬¸ì˜ ì´ˆë¡ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ì´ ë…¼ë¬¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤: "),
            ("ì´ ì—°êµ¬ì˜ ì£¼ìš” ê¸°ì—¬ì ì€ ë¬´ì—‡ì¸ê°€ìš”?", "ì£¼ìš” ê¸°ì—¬ì : "),
        ],
        "introduction": [
            ("ì´ ë…¼ë¬¸ì˜ ì—°êµ¬ ë°°ê²½ê³¼ ë™ê¸°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ì—°êµ¬ ë°°ê²½: "),
            ("ì´ ì—°êµ¬ê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ”?", "í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ: "),
        ],
        "method": [
            ("ì´ ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ë°©ë²•ë¡ : "),
            ("ì œì•ˆí•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ”?", "í•µì‹¬ ì•„ì´ë””ì–´: "),
        ],
        "experiment": [
            ("ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.", "ì‹¤í—˜ ê²°ê³¼: "),
            ("ì–´ë–¤ ë°ì´í„°ì…‹ê³¼ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í–ˆë‚˜ìš”?", "ì‚¬ìš©ëœ ë°ì´í„°ì…‹ê³¼ ë©”íŠ¸ë¦­: "),
        ],
        "conclusion": [
            ("ì´ ë…¼ë¬¸ì˜ ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?", "ê²°ë¡ : "),
            ("í–¥í›„ ì—°êµ¬ ë°©í–¥ì€?", "í–¥í›„ ì—°êµ¬: "),
        ],
    }
    
    for section_name, content in sections.items():
        if len(content) < 50:
            continue
            
        # í•´ë‹¹ ì„¹ì…˜ì˜ instruction ì°¾ê¸°
        for key, instructions in section_instructions.items():
            if key in section_name.lower():
                for instruction, prefix in instructions:
                    dataset.append({
                        "instruction": instruction,
                        "input": content[:2000],
                        "output": prefix + content[:1000]
                    })
                break
        
        # ê¸°ë³¸ ìš”ì•½ instruction
        dataset.append({
            "instruction": f"ë‹¤ìŒ {section_name} ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "input": content[:2000],
            "output": f"ì´ ì„¹ì…˜ì˜ ìš”ì•½: {content[:800]}"
        })
    
    return dataset

# ê·œì¹™ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„± (ë¹ ë¦„)
dataset_rules = create_qa_dataset_rule_based(chunks)
print(f"âœ… ê·œì¹™ ê¸°ë°˜ ë°ì´í„°ì…‹: {len(dataset_rules)}ê°œ ìƒ˜í”Œ")

# ì„¹ì…˜ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„±
dataset_sections = create_instruction_dataset(sections)
print(f"âœ… ì„¹ì…˜ ê¸°ë°˜ ë°ì´í„°ì…‹: {len(dataset_sections)}ê°œ ìƒ˜í”Œ")

# í•©ì¹˜ê¸°
training_data = dataset_rules + dataset_sections
print(f"âœ… ì´ í•™ìŠµ ë°ì´í„°: {len(training_data)}ê°œ ìƒ˜í”Œ")

# %% [Cell 7] ë°ì´í„°ì…‹ ì €ì¥ ë° í™•ì¸
# ============================================

# JSONìœ¼ë¡œ ì €ì¥
with open("paper_dataset.json", "w", encoding="utf-8") as f:
    json.dump(training_data, f, ensure_ascii=False, indent=2)

print("âœ… paper_dataset.json ì €ì¥ ì™„ë£Œ!")

# ìƒ˜í”Œ í™•ì¸
print("\nğŸ“‹ ë°ì´í„°ì…‹ ìƒ˜í”Œ:")
print("=" * 60)
for i, sample in enumerate(training_data[:3]):
    print(f"\n[ìƒ˜í”Œ {i+1}]")
    print(f"Instruction: {sample['instruction'][:100]}...")
    print(f"Input: {sample['input'][:150]}...")
    print(f"Output: {sample['output'][:150]}...")
    print("-" * 60)

# %% [Cell 8] ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì •
# ============================================
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
import torch

# ì„¤ì •
max_seq_length = 2048
dtype = None
load_in_4bit = True

# ğŸ”¥ GPT ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì„ íƒ (ì›í•˜ëŠ” ëª¨ë¸ ì£¼ì„ í•´ì œ)
model_name = "unsloth/Meta-Llama-3.1-8B"        # Llama 3.1 8B
# model_name = "unsloth/mistral-7b-v0.3"        # Mistral 7B
# model_name = "unsloth/gemma-2-9b"              # Gemma 2 9B
# model_name = "unsloth/Qwen2.5-7B"              # Qwen 2.5 7B
# model_name = "unsloth/Phi-3.5-mini-instruct"  # Phi 3.5 Mini

print(f"ğŸš€ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# LoRA ì–´ëŒ‘í„° ì„¤ì •
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

print("âœ… LoRA ì–´ëŒ‘í„° ì„¤ì • ì™„ë£Œ!")

# %% [Cell 9] ë°ì´í„° í¬ë§·íŒ…
# ============================================

# Alpaca í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    texts = []
    for instruction, input_text, output in zip(
        examples["instruction"],
        examples["input"],
        examples["output"]
    ):
        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

# Dataset ê°ì²´ë¡œ ë³€í™˜
dataset = Dataset.from_list(training_data)
dataset = dataset.map(formatting_prompts_func, batched=True)

print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! ({len(dataset)}ê°œ ìƒ˜í”Œ)")

# %% [Cell 10] í•™ìŠµ ì‹¤í–‰
# ============================================

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        num_train_epochs=3,          # ì „ì²´ ë°ì´í„° 3ë²ˆ ë°˜ë³µ
        # max_steps=100,             # ë˜ëŠ” ìµœëŒ€ ìŠ¤í… ìˆ˜ ì§€ì •
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",  # cosine ìŠ¤ì¼€ì¤„ëŸ¬
        seed=3407,
        output_dir="outputs",
        report_to="none",
        save_strategy="epoch",
    ),
)

print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("=" * 60)

trainer_stats = trainer.train()

print("\n" + "=" * 60)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print(f"â±ï¸ ì´ í•™ìŠµ ì‹œê°„: {trainer_stats.metrics['train_runtime']:.2f}ì´ˆ")
print(f"ğŸ“‰ ìµœì¢… Loss: {trainer_stats.metrics['train_loss']:.4f}")

# %% [Cell 11] ëª¨ë¸ ì €ì¥
# ============================================

# LoRA ì–´ëŒ‘í„° ì €ì¥
model.save_pretrained("paper_lora_model")
tokenizer.save_pretrained("paper_lora_model")

print("âœ… LoRA ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (paper_lora_model)")

# Google Driveì— ì €ì¥
from google.colab import drive
drive.mount('/content/drive')

import shutil
shutil.copytree("paper_lora_model", "/content/drive/MyDrive/paper_lora_model")
print("âœ… Google Driveì— ì €ì¥ ì™„ë£Œ!")

# %% [Cell 12] ì¶”ë¡  í…ŒìŠ¤íŠ¸
# ============================================

# ì¶”ë¡  ëª¨ë“œ ì „í™˜
FastLanguageModel.for_inference(model)

def ask_paper(question, context=""):
    """ë…¼ë¬¸ ê´€ë ¨ ì§ˆë¬¸í•˜ê¸°"""
    if context:
        input_text = f"í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}"
    else:
        input_text = question
    
    prompt = alpaca_prompt.format(
        "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
        input_text,
        ""
    )
    
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        use_cache=True,
    )
    
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    
    if "### Response:" in response:
        return response.split("### Response:")[-1].strip()
    return response


# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
print("=" * 60)
print("ğŸ¤– ëª¨ë¸ í…ŒìŠ¤íŠ¸")
print("=" * 60)

test_questions = [
    "ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
]

# ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
context = chunks[0] if chunks else ""

for q in test_questions:
    print(f"\nâ“ ì§ˆë¬¸: {q}")
    answer = ask_paper(q, context)
    print(f"ğŸ’¬ ë‹µë³€: {answer[:500]}...")
    print("-" * 40)

# %% [Cell 13] (ì„ íƒ) LLMìœ¼ë¡œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ìƒì„±
# ============================================
# ëª¨ë¸ ë¡œë“œ í›„ ì‹¤í–‰í•˜ë©´ ë” ì¢‹ì€ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ ìƒì„±

# dataset_llm = create_qa_dataset_with_llm(chunks[:20], model, tokenizer, alpaca_prompt)
# print(f"âœ… LLM ìƒì„± ë°ì´í„°ì…‹: {len(dataset_llm)}ê°œ")

# # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
# training_data_enhanced = training_data + dataset_llm

# %% [Cell 14] (ì„ íƒ) GGUF í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
# ============================================
# llama.cpp, Ollama ë“±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

# q4_k_m: ì¢‹ì€ í’ˆì§ˆ + ì‘ì€ ìš©ëŸ‰ (ê¶Œì¥)
# model.save_pretrained_gguf("paper_model_gguf", tokenizer, quantization_method="q4_k_m")

# q8_0: ë” ë†’ì€ í’ˆì§ˆ
# model.save_pretrained_gguf("paper_model_gguf_q8", tokenizer, quantization_method="q8_0")

# %% [Cell 15] (ì„ íƒ) Hugging Face Hub ì—…ë¡œë“œ
# ============================================

# from huggingface_hub import login
# login(token="YOUR_HF_TOKEN")  # https://huggingface.co/settings/tokens

# model.push_to_hub("your-username/paper-finetuned-llama")
# tokenizer.push_to_hub("your-username/paper-finetuned-llama")

print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("=" * 60)
print("ğŸ“ ì €ì¥ëœ íŒŒì¼:")
print("   - paper_dataset.json (í•™ìŠµ ë°ì´í„°)")
print("   - paper_lora_model/ (LoRA ì–´ëŒ‘í„°)")
print("   - Google Driveì— ë°±ì—…ë¨")

# ============================================
# ê³ ê¸‰ PDF â†’ ë°ì´í„°ì…‹ ìƒì„±ê¸°
# LLMì„ í™œìš©í•œ ê³ í’ˆì§ˆ Q&A ìƒì„±
# ============================================

import pdfplumber
import json
import re
from tqdm import tqdm
from typing import List, Dict

class PaperDatasetGenerator:
    """ë…¼ë¬¸ PDFì—ì„œ í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model=None, tokenizer=None):
        self.model = model
        self.tokenizer = tokenizer
        self.alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        full_text = ""
        
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    full_text += text + "\n\n"
        
        return self._clean_text(full_text)
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # í•˜ì´í”ˆìœ¼ë¡œ ëŠê¸´ ë‹¨ì–´ ì—°ê²°
        text = re.sub(r'(\w+)-\n(\w+)', r'\1\2', text)
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r' {2,}', ' ', text)
        return text.strip()
    
    def split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [c for c in chunks if len(c) > 100]
    
    def extract_sections(self, text: str) -> Dict[str, str]:
        """ë…¼ë¬¸ ì„¹ì…˜ ì¶”ì¶œ"""
        section_keywords = {
            'abstract': ['abstract'],
            'introduction': ['introduction', '1. introduction', '1 introduction'],
            'related_work': ['related work', 'background', '2. related', '2 related'],
            'method': ['method', 'methodology', 'approach', 'model', '3. method', '3 method'],
            'experiment': ['experiment', 'evaluation', 'result', '4. experiment', '4 experiment'],
            'discussion': ['discussion', '5. discussion'],
            'conclusion': ['conclusion', '6. conclusion', 'summary'],
        }
        
        sections = {}
        lines = text.lower().split('\n')
        
        current_section = 'header'
        current_content = []
        
        for line in text.split('\n'):
            line_lower = line.lower().strip()
            
            found = False
            for section_name, keywords in section_keywords.items():
                if any(kw in line_lower for kw in keywords) and len(line_lower) < 50:
                    if current_content:
                        sections[current_section] = '\n'.join(current_content)
                    current_section = section_name
                    current_content = []
                    found = True
                    break
            
            if not found:
                current_content.append(line)
        
        if current_content:
            sections[current_section] = '\n'.join(current_content)
        
        return sections
    
    def generate_qa_pairs_rule_based(self, chunks: List[str]) -> List[Dict]:
        """ê·œì¹™ ê¸°ë°˜ Q&A ìŒ ìƒì„±"""
        dataset = []
        
        templates = [
            {
                "type": "summary",
                "instruction": "ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "output_template": "ìš”ì•½: {content}"
            },
            {
                "type": "explain",
                "instruction": "ë‹¤ìŒ í•™ìˆ  ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "output_template": "ì„¤ëª…: {content}"
            },
            {
                "type": "keypoint",
                "instruction": "ë‹¤ìŒ ë‚´ìš©ì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.",
                "output_template": "í•µì‹¬ í¬ì¸íŠ¸:\n{content}"
            },
            {
                "type": "qa",
                "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
                "output_template": "{content}"
            },
        ]
        
        for chunk in chunks:
            # ìš”ì•½ íƒœìŠ¤í¬
            dataset.append({
                "instruction": "ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "input": chunk,
                "output": f"ìš”ì•½: {self._create_summary(chunk)}"
            })
            
            # ì„¤ëª… íƒœìŠ¤í¬
            dataset.append({
                "instruction": "ë‹¤ìŒ í•™ìˆ  ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "input": chunk,
                "output": f"ì„¤ëª…: {self._create_explanation(chunk)}"
            })
        
        return dataset
    
    def _create_summary(self, text: str, max_len: int = 200) -> str:
        """ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì²« ë¬¸ì¥ë“¤ ì¶”ì¶œ)"""
        sentences = re.split(r'[.!?]\s+', text)
        summary = ""
        for s in sentences:
            if len(summary) + len(s) < max_len:
                summary += s + ". "
            else:
                break
        return summary.strip() or text[:max_len]
    
    def _create_explanation(self, text: str, max_len: int = 300) -> str:
        """ì„¤ëª… ìƒì„±"""
        return text[:max_len].strip() + "..."
    
    def generate_qa_pairs_with_llm(self, chunks: List[str], num_questions: int = 2) -> List[Dict]:
        """LLMì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ Q&A ìƒì„±"""
        if not self.model or not self.tokenizer:
            raise ValueError("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        from unsloth import FastLanguageModel
        FastLanguageModel.for_inference(self.model)
        
        dataset = []
        
        for chunk in tqdm(chunks, desc="Q&A ìƒì„± ì¤‘"):
            # 1. ì§ˆë¬¸ ìƒì„±
            questions = self._generate_questions(chunk, num_questions)
            
            # 2. ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
            for question in questions:
                answer = self._generate_answer(chunk, question)
                
                dataset.append({
                    "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.",
                    "input": f"í…ìŠ¤íŠ¸: {chunk}\n\nì§ˆë¬¸: {question}",
                    "output": answer
                })
        
        return dataset
    
    def _generate_questions(self, context: str, num_questions: int = 2) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ ìƒì„±"""
        prompt = self.alpaca_prompt.format(
            f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì´ ë‚´ìš©ì— ëŒ€í•´ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” í•µì‹¬ ì§ˆë¬¸ {num_questions}ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ê° ì§ˆë¬¸ì€ ìƒˆ ì¤„ì— ë²ˆí˜¸ì™€ í•¨ê»˜ ì‘ì„±í•˜ì„¸ìš”.",
            context[:1500],
            ""
        )
        
        inputs = self.tokenizer([prompt], return_tensors="pt").to("cuda")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.8,
            do_sample=True,
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "### Response:" in response:
            response = response.split("### Response:")[-1]
        
        # ì§ˆë¬¸ íŒŒì‹±
        questions = []
        for line in response.split('\n'):
            line = re.sub(r'^[\d]+[.)\s]+', '', line.strip())
            if line and '?' in line:
                questions.append(line)
        
        return questions[:num_questions]
    
    def _generate_answer(self, context: str, question: str) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        prompt = self.alpaca_prompt.format(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.",
            f"í…ìŠ¤íŠ¸: {context[:1500]}\n\nì§ˆë¬¸: {question}",
            ""
        )
        
        inputs = self.tokenizer([prompt], return_tensors="pt").to("cuda")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.3,
            do_sample=True,
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "### Response:" in response:
            response = response.split("### Response:")[-1].strip()
        
        return response
    
    def generate_section_based_dataset(self, sections: Dict[str, str]) -> List[Dict]:
        """ì„¹ì…˜ë³„ instruction ë°ì´í„°ì…‹ ìƒì„±"""
        dataset = []
        
        section_prompts = {
            'abstract': [
                ("ì´ ë…¼ë¬¸ì˜ ì´ˆë¡ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ì´ˆë¡ ë‚´ìš©"),
                ("ì´ ì—°êµ¬ì˜ ëª©ì ê³¼ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.", "ì—°êµ¬ ìš”ì•½"),
            ],
            'introduction': [
                ("ì´ ë…¼ë¬¸ì˜ ì—°êµ¬ ë°°ê²½ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ì—°êµ¬ ë°°ê²½"),
                ("ì´ ì—°êµ¬ê°€ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "ì—°êµ¬ ë¬¸ì œ"),
            ],
            'method': [
                ("ì œì•ˆí•˜ëŠ” ë°©ë²•ë¡ ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "ë°©ë²•ë¡  ì„¤ëª…"),
                ("í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ì ‘ê·¼ ë°©ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?", "í•µì‹¬ ì ‘ê·¼"),
            ],
            'experiment': [
                ("ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.", "ì‹¤í—˜ ìš”ì•½"),
                ("ì£¼ìš” ì‹¤í—˜ ê²°ê³¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "ì‹¤í—˜ ê²°ê³¼"),
            ],
            'conclusion': [
                ("ì´ ë…¼ë¬¸ì˜ ê²°ë¡ ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.", "ê²°ë¡ "),
                ("í–¥í›„ ì—°êµ¬ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?", "í–¥í›„ ì—°êµ¬"),
            ],
        }
        
        for section_name, content in sections.items():
            if len(content) < 100:
                continue
            
            prompts = section_prompts.get(section_name, [
                (f"{section_name} ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.", "ìš”ì•½")
            ])
            
            for instruction, prefix in prompts:
                dataset.append({
                    "instruction": instruction,
                    "input": content[:2000],
                    "output": f"{prefix}: {content[:1000]}"
                })
        
        return dataset
    
    def create_full_dataset(self, pdf_path: str, use_llm: bool = False) -> List[Dict]:
        """ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸"""
        print(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {pdf_path}")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        print(f"   í…ìŠ¤íŠ¸ ì¶”ì¶œ: {len(text)} ë¬¸ì")
        
        # 2. ì²­í¬ ë¶„í• 
        chunks = self.split_into_chunks(text)
        print(f"   ì²­í¬ ë¶„í• : {len(chunks)}ê°œ")
        
        # 3. ì„¹ì…˜ ì¶”ì¶œ
        sections = self.extract_sections(text)
        print(f"   ì„¹ì…˜ ë°œê²¬: {list(sections.keys())}")
        
        # 4. ë°ì´í„°ì…‹ ìƒì„±
        dataset = []
        
        # ê·œì¹™ ê¸°ë°˜
        dataset.extend(self.generate_qa_pairs_rule_based(chunks))
        print(f"   ê·œì¹™ ê¸°ë°˜ ë°ì´í„°: {len(dataset)}ê°œ")
        
        # ì„¹ì…˜ ê¸°ë°˜
        section_data = self.generate_section_based_dataset(sections)
        dataset.extend(section_data)
        print(f"   ì„¹ì…˜ ê¸°ë°˜ ë°ì´í„°: {len(section_data)}ê°œ")
        
        # LLM ê¸°ë°˜ (ì„ íƒ)
        if use_llm and self.model:
            llm_data = self.generate_qa_pairs_with_llm(chunks[:10])
            dataset.extend(llm_data)
            print(f"   LLM ê¸°ë°˜ ë°ì´í„°: {len(llm_data)}ê°œ")
        
        print(f"âœ… ì´ ë°ì´í„°ì…‹: {len(dataset)}ê°œ")
        return dataset
    
    def save_dataset(self, dataset: List[Dict], output_path: str):
        """ë°ì´í„°ì…‹ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")


# ============================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================
if __name__ == "__main__":
    # 1. ê¸°ë³¸ ì‚¬ìš© (ê·œì¹™ ê¸°ë°˜ë§Œ)
    generator = PaperDatasetGenerator()
    dataset = generator.create_full_dataset("paper.pdf", use_llm=False)
    generator.save_dataset(dataset, "paper_dataset.json")
    
    # 2. LLMê³¼ í•¨ê»˜ ì‚¬ìš© (ë” ë†’ì€ í’ˆì§ˆ)
    # from unsloth import FastLanguageModel
    # model, tokenizer = FastLanguageModel.from_pretrained(...)
    # generator = PaperDatasetGenerator(model, tokenizer)
    # dataset = generator.create_full_dataset("paper.pdf", use_llm=True)
